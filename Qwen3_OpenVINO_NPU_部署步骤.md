# Qwen3-8B OpenVINO NPU éƒ¨ç½²æ­¥éª¤

## ğŸ“‹ å®Œæ•´æµç¨‹

### æ­¥éª¤ 1: å®‰è£…ä¾èµ–

```cmd
pip install optimum-intel openvino-genai transformers torch
```

### æ­¥éª¤ 2: å¯¼å‡ºå¹¶é‡åŒ–æ¨¡å‹ï¼ˆINT4ï¼‰

**è¿™ä¸€æ­¥ä¼šè‡ªåŠ¨ä¸‹è½½ Qwen3-8B æ¨¡å‹å¹¶è½¬æ¢ä¸º OpenVINO æ ¼å¼**

```cmd
optimum-cli export openvino --model Qwen/Qwen3-8B --task text-generation-with-past --weight-format int4 --group-size 128 --ratio 0.8 Qwen3-8B-int4-ov
```

**å‚æ•°è¯´æ˜**:
- `--model Qwen/Qwen3-8B`: ä» HuggingFace ä¸‹è½½ Qwen3-8B
- `--task text-generation-with-past`: æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
- `--weight-format int4`: INT4 é‡åŒ–
- `--group-size 128`: é‡åŒ–ç»„å¤§å°
- `--ratio 0.8`: 80% å±‚ä½¿ç”¨ INT4ï¼Œ20% ä½¿ç”¨ INT8
- `Qwen3-8B-int4-ov`: è¾“å‡ºç›®å½•

**é¢„è®¡æ—¶é—´**: 30-60 åˆ†é’Ÿï¼ˆå–å†³äºç½‘é€Ÿå’Œç¡¬ä»¶ï¼‰

### æ­¥éª¤ 3: è¿è¡Œæ¨¡å‹ï¼ˆNPUï¼‰

åˆ›å»º `run_qwen3_npu.py`:

```python
import openvino_genai as ov_genai

# ä½¿ç”¨ NPU
pipe = ov_genai.LLMPipeline("Qwen3-8B-int4-ov", "NPU")

# æµ‹è¯•
print(pipe.generate("What is artificial intelligence?", max_new_tokens=200))
```

è¿è¡Œ:
```cmd
python run_qwen3_npu.py
```

---

## ğŸ”§ å…¶ä»–é‡åŒ–é€‰é¡¹

### INT8 é‡åŒ–ï¼ˆæ›´é«˜è´¨é‡ï¼‰

```cmd
optimum-cli export openvino --model Qwen/Qwen3-8B --task text-generation-with-past --weight-format int8 Qwen3-8B-int8-ov
```

### FP16ï¼ˆæœ€é«˜è´¨é‡ï¼Œæœ€å¤§ï¼‰

```cmd
optimum-cli export openvino --model Qwen/Qwen3-8B --task text-generation-with-past --weight-format fp16 Qwen3-8B-fp16-ov
```

---

## ğŸ¯ è®¾å¤‡é€‰é¡¹

```python
# NPUï¼ˆæ¨èï¼ŒIntel Arc NPUï¼‰
pipe = ov_genai.LLMPipeline("Qwen3-8B-int4-ov", "NPU")

# GPUï¼ˆIntel Arc GPUï¼‰
pipe = ov_genai.LLMPipeline("Qwen3-8B-int4-ov", "GPU")

# CPUï¼ˆå¤‡ç”¨ï¼‰
pipe = ov_genai.LLMPipeline("Qwen3-8B-int4-ov", "CPU")

# æ··åˆï¼ˆNPU + GPU + CPUï¼‰
pipe = ov_genai.LLMPipeline("Qwen3-8B-int4-ov", "MULTI:NPU,GPU,CPU")
```

---

## ğŸ“Š é¢„æœŸç»“æœ

### æ¨¡å‹å¤§å°
- **INT4**: ~4-5 GB
- **INT8**: ~8-9 GB  
- **FP16**: ~16-17 GB

### æ€§èƒ½ï¼ˆIntel Arc 140Vï¼‰
- **NPU**: æœ€å¿«ï¼Œæœ€çœç”µ
- **GPU**: å¿«ï¼Œæ˜¾å­˜å ç”¨é«˜
- **CPU**: è¾ƒæ…¢

---

## ğŸš€ ä¸€é”®è¿è¡Œè„šæœ¬

åˆ›å»º `deploy_qwen3_npu.bat`:

```bat
@echo off
echo æ­¥éª¤ 1: å®‰è£…ä¾èµ–
pip install optimum-intel openvino-genai transformers torch

echo.
echo æ­¥éª¤ 2: å¯¼å‡ºå¹¶é‡åŒ–æ¨¡å‹ï¼ˆINT4ï¼‰
echo è¿™å°†éœ€è¦ 30-60 åˆ†é’Ÿ...
optimum-cli export openvino --model Qwen/Qwen3-8B --task text-generation-with-past --weight-format int4 --group-size 128 --ratio 0.8 Qwen3-8B-int4-ov

echo.
echo æ­¥éª¤ 3: æµ‹è¯•æ¨¡å‹
python -c "import openvino_genai as ov_genai; pipe = ov_genai.LLMPipeline('Qwen3-8B-int4-ov', 'NPU'); print(pipe.generate('Hello!', max_new_tokens=50))"

pause
```

---

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡å‹**ï¼ˆçº¦ 16GBï¼‰
2. **é‡åŒ–è¿‡ç¨‹éœ€è¦æ—¶é—´**ï¼ˆ30-60 åˆ†é’Ÿï¼‰
3. **ç¡®ä¿æœ‰è¶³å¤Ÿç£ç›˜ç©ºé—´**ï¼ˆè‡³å°‘ 30GBï¼‰
4. **NPU éœ€è¦æœ€æ–°é©±åŠ¨**

---

## ğŸ”— å‚è€ƒæ–‡æ¡£

- [Intel å®˜æ–¹æ–‡æ¡£](https://newsroom.intel.com/zh-cn/äººå·¥æ™ºèƒ½/è‹±ç‰¹å°”ç¬¬ä¸€æ—¶é—´æ·±åº¦ä¼˜åŒ–qwen3å¤§æ¨¡å‹å‡çº§ai-pcèƒ½åŠ›èµ‹èƒ½)
- [OpenVINO NPU æ–‡æ¡£](https://docs.openvino.ai/2025/openvino-workflow-generative/inference-with-genai/inference-with-genai-on-npu.html)

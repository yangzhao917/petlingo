# Qwen3 Intel AI PC 部署项目

## 📖 项目概述

本项目提供了在 Intel AI PC（特别是 Intel Arc 140V GPU/NPU）上部署和运行 Qwen3 大语言模型的完整解决方案。项目包含多种部署方式，从简单易用的 Intel Ollama 到高性能的 OpenVINO NPU 推理，满足不同场景的需求。

## 🎯 核心特性

- **多种部署方案**: Intel Ollama、OpenVINO NPU、OpenVINO GPU/CPU
- **硬件加速优化**: 充分利用 Intel Arc GPU 和 NPU 的 AI 加速能力
- **模型量化支持**: INT4/INT8 量化，大幅降低显存占用和提升推理速度
- **MoE 架构优化**: 针对 Mixture of Experts 模型的显存优化配置
- **一键部署脚本**: 自动化安装、配置和运行流程
- **多模态支持**: 支持文本生成、视觉理解等多种任务

## 🖥️ 目标硬件

- **GPU**: Intel Arc 140V (16GB VRAM)
- **NPU**: Intel AI Boost (集成神经处理单元)
- **CPU**: Intel Core Ultra 处理器
- **内存**: 建议 16GB 以上

## 📦 支持的模型

### Qwen 系列模型
- **Qwen3-8B**: 最新版本，性能优秀
- **Qwen2.5-MoE-7B**: MoE 架构，平衡性能和资源占用
- **Qwen2.5-MoE-14B**: 更强性能
- **Qwen3-30B**: 大规模模型，适合专业应用
- **Qwen3-4B**: 轻量级模型，快速推理

### 其他模型
- **Phi-3.5**: Microsoft 小型高效模型
- **Qwen3-VL**: 视觉语言多模态模型
- **Paraformer**: 语音识别模型（FunASR）

## 🚀 部署方案对比

| 方案 | 难度 | 性能 | 显存占用 | 推荐场景 |
|------|------|------|----------|----------|
| **Intel Ollama** | ⭐ 简单 | 快 | 中等 | 快速开始、日常使用 |
| **OpenVINO NPU** | ⭐⭐⭐ 复杂 | 最快 | 最低 | 生产环境、高性能需求 |
| **OpenVINO GPU** | ⭐⭐ 中等 | 很快 | 中等 | 平衡性能和易用性 |

## 📂 项目结构

```
├── 📁 Intel Ollama 部署
│   ├── ollama-intel-2.3.0b20250923-win/    # Intel Ollama 主程序
│   ├── setup_qwen3_auto.bat                # 一键部署脚本
│   ├── run_qwen3.bat                       # 运行模型脚本
│   ├── setup_intel_ollama.py               # Python 安装脚本
│   └── Intel_Ollama_使用指南.md            # 详细使用文档
│
├── 📁 OpenVINO NPU 部署
│   ├── deploy_qwen3_npu.py                 # NPU 部署主脚本
│   ├── setup_qwen3_npu_openvino.py         # 自动化设置脚本
│   ├── test_qwen3_npu.py                   # NPU 测试脚本
│   ├── test_qwen3_4b_npu.py                # Qwen3-4B NPU 测试
│   ├── test_qwen3_4b_nf4_npu.py            # INT4 量化测试
│   └── Qwen3_OpenVINO_NPU_部署步骤.md      # NPU 部署文档
│
├── 📁 模型转换工具
│   ├── convert_qwen3.py                    # 模型格式转换
│   ├── convert_qwen3_to_openvino.py        # 转换为 OpenVINO 格式
│   ├── download_model_manual.py            # 手动下载模型
│   └── quantize_qwen_model.bat             # 模型量化脚本
│
├── 📁 已下载模型
│   ├── qwen3-8b-int4-cw-ov/                # Qwen3-8B INT4 量化模型
│   ├── Qwen3-8B-original/                  # Qwen3-8B 原始模型
│   ├── qwen3_original/                     # 其他 Qwen3 模型
│   ├── ov_qwen3vl8b/                       # Qwen3-VL 视觉模型
│   └── ov_phi35/                           # Phi-3.5 模型
│
├── 📁 多模态支持
│   ├── FunASR-main/                        # 语音识别框架
│   ├── ov_paraformer/                      # Paraformer 语音模型
│   └── GPT-SoVITS-For-Intel/               # 语音合成（Intel 优化）
│
└── 📁 文档和示例
    ├── README_Qwen3_部署方案.md            # 部署方案总览
    ├── README_Intel_Ollama.md              # Ollama 快速开始
    ├── README_Qwen3_NPU.md                 # NPU 部署指南
    ├── OpenVINO_部署说明.md                # OpenVINO 详细说明
    └── 部署完成_README.md                  # 部署完成后的说明
```

## 🎓 快速开始

### 方案 1: Intel Ollama（推荐新手）

最简单的部署方式，适合快速体验和日常使用。

```cmd
# 1. 一键部署
setup_qwen3_auto.bat

# 2. 运行模型
run_qwen3.bat
```

**特点**:
- ✅ 安装简单，5 分钟内完成
- ✅ 自动从 ModelScope 下载（国内快）
- ✅ 自动配置显存优化
- ✅ 支持 GPU + CPU 混合推理
- ✅ 内置 MoE 模型优化

### 方案 2: OpenVINO NPU（推荐高级用户）

最高性能的部署方式，充分利用 Intel NPU 的 AI 加速能力。

```cmd
# 1. 自动部署
python deploy_qwen3_npu.py

# 2. 或手动部署
python setup_qwen3_npu_openvino.py
```

**特点**:
- ✅ 最快的推理速度
- ✅ 最低的功耗和显存占用
- ✅ INT4 量化，模型体积小
- ✅ 支持 NPU + GPU + CPU 混合推理
- ⚠️ 需要安装 Intel NPU 驱动

### 方案 3: 手动部署

如果你想完全控制部署过程：

```cmd
# 1. 安装依赖
pip install optimum-intel openvino-genai transformers torch

# 2. 导出模型
optimum-cli export openvino --model Qwen/Qwen3-8B \
    --task text-generation-with-past \
    --weight-format int4 \
    --group-size 128 \
    Qwen3-8B-int4-ov

# 3. 运行推理
python run_qwen3_openvino.py
```

## 💡 使用示例

### Python API 调用

```python
import openvino_genai as ov_genai

# 加载模型（NPU）
pipe = ov_genai.LLMPipeline("qwen3-8b-int4-cw-ov", "NPU")

# 生成文本
response = pipe.generate(
    "什么是人工智能？",
    max_length=200,
    temperature=0.7
)
print(response)
```

### Ollama API 调用

```bash
# 命令行
ollama run qwen3:30b "介绍一下量子计算"

# HTTP API
curl http://localhost:11434/api/generate -d '{
  "model": "qwen3:30b",
  "prompt": "什么是机器学习？"
}'
```

### 流式输出

```python
# OpenVINO 流式生成
for token in pipe.generate_stream("写一首诗", max_length=100):
    print(token, end="", flush=True)
```

## ⚙️ 性能优化配置

### Intel Ollama 优化

```cmd
# 显存优化（推荐 16GB 显存）
set OLLAMA_NUM_CTX=8192          # 上下文长度
set OLLAMA_NUM_PARALLEL=1        # 并行请求数
set OLLAMA_SET_OT=exps=CPU       # MoE experts 在 CPU

# 启动服务
cd ollama-intel-2.3.0b20250923-win
start-ollama.bat
```

### OpenVINO 设备选择

```python
# NPU（最快，最省电）
pipe = ov_genai.LLMPipeline(model_path, "NPU")

# GPU（快，显存占用高）
pipe = ov_genai.LLMPipeline(model_path, "GPU")

# CPU（兼容性最好）
pipe = ov_genai.LLMPipeline(model_path, "CPU")

# 自动选择（推荐）
pipe = ov_genai.LLMPipeline(model_path, "AUTO")

# 混合推理（NPU + GPU + CPU）
pipe = ov_genai.LLMPipeline(model_path, "MULTI:NPU,GPU,CPU")
```

### 模型量化选项

| 量化类型 | 模型大小 | 推理速度 | 质量 | 推荐场景 |
|---------|---------|---------|------|----------|
| **INT4** | ~4-5GB | 最快 | 良好 | 生产环境（推荐） |
| **INT8** | ~8-9GB | 快 | 很好 | 平衡性能和质量 |
| **FP16** | ~16GB | 中等 | 最好 | 研究和开发 |
| **FP32** | ~32GB | 慢 | 完美 | 基准测试 |

## 📊 性能基准

### Intel Arc 140V (16GB) 测试结果

| 模型 | 量化 | 设备 | 加载时间 | 推理速度 | 显存占用 |
|------|------|------|----------|----------|----------|
| Qwen3-8B | INT4 | NPU | ~30s | ~40 tokens/s | ~4GB |
| Qwen3-8B | INT4 | GPU | ~20s | ~35 tokens/s | ~5GB |
| Qwen3-4B | INT4 | NPU | ~15s | ~60 tokens/s | ~3GB |
| Qwen2.5-MoE-7B | INT4 | GPU+CPU | ~25s | ~30 tokens/s | ~5GB |

*注: 首次运行需要编译，后续运行会更快*

## 🔧 常见问题解决

### 1. 显存不足 (OOM)

**症状**: 模型加载失败，提示内存不足

**解决方案**:
```cmd
# Ollama 优化
set OLLAMA_NUM_PARALLEL=1
set OLLAMA_NUM_CTX=4096
set OLLAMA_SET_OT=exps=CPU

# 或使用更小的模型
ollama run qwen2.5-moe:7b  # 而非 30b
```

### 2. NPU 不工作

**症状**: NPU 设备不可用或推理失败

**解决方案**:
1. 检查 Intel NPU 驱动是否安装
2. 更新到最新驱动版本
3. 在设备管理器中查看 "神经处理器"
4. 尝试使用 GPU 或 AUTO 设备

```python
# 回退到 GPU
pipe = ov_genai.LLMPipeline(model_path, "GPU")
```

### 3. 下载速度慢

**症状**: 模型下载缓慢或超时

**解决方案**:
- Intel Ollama 默认使用 ModelScope（国内快）
- 使用镜像站点或代理
- 手动下载模型文件

```cmd
# 使用 ModelScope（默认）
set OLLAMA_MODEL_SOURCE=modelscope

# 或使用 Ollama 官方库
set OLLAMA_MODEL_SOURCE=ollama
```

### 4. 模型加载慢

**症状**: 首次加载需要很长时间

**原因**: OpenVINO 首次运行需要编译模型

**解决方案**:
- 这是正常现象，首次编译约 1-5 分钟
- 编译后会缓存，后续加载很快
- 可以预先编译模型

### 5. 推理结果质量差

**症状**: 生成的文本质量不佳

**解决方案**:
```python
# 调整生成参数
response = pipe.generate(
    prompt,
    max_length=200,
    temperature=0.7,      # 降低随机性
    top_p=0.9,           # Top-p 采样
    top_k=50,            # Top-k 采样
    repetition_penalty=1.1  # 减少重复
)
```

## 🛠️ 高级功能

### 1. 批量推理

```python
# 批量处理多个请求
prompts = [
    "什么是人工智能？",
    "介绍一下量子计算",
    "解释深度学习原理"
]

for prompt in prompts:
    response = pipe.generate(prompt, max_length=200)
    print(f"Q: {prompt}\nA: {response}\n")
```

### 2. 自定义 Modelfile

```dockerfile
# 创建 Modelfile
FROM qwen3:30b

# 设置系统提示词
SYSTEM """
你是一个专业的 AI 助手，擅长回答技术问题。
请用简洁、准确的语言回答用户的问题。
"""

# 设置参数
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 50
```

```cmd
# 创建自定义模型
ollama create my-qwen3 -f Modelfile

# 运行自定义模型
ollama run my-qwen3
```

### 3. 多模态推理（视觉）

```python
# 使用 Qwen3-VL 进行图像理解
import openvino_genai as ov_genai

pipe = ov_genai.VLMPipeline("ov_qwen3vl8b", "GPU")

# 图像 + 文本输入
response = pipe.generate(
    image="sample.png",
    prompt="描述这张图片中的内容"
)
print(response)
```

### 4. 语音识别集成

```python
# 使用 FunASR Paraformer 进行语音识别
from funasr import AutoModel

model = AutoModel(model="ov_paraformer")
result = model.generate(input="audio.wav")
print(result[0]["text"])
```

## 📚 技术栈

### 核心框架
- **OpenVINO**: Intel 深度学习推理框架
- **OpenVINO GenAI**: 生成式 AI 推理工具包
- **Optimum Intel**: HuggingFace 模型优化工具
- **Intel Ollama**: Intel 优化的 Ollama 版本

### 模型框架
- **Transformers**: HuggingFace 模型库
- **PyTorch**: 深度学习框架
- **ONNX**: 模型交换格式

### 量化工具
- **NNCF**: Neural Network Compression Framework
- **Optimum**: 模型优化和量化

## 🔗 相关资源

### 官方文档
- [OpenVINO 文档](https://docs.openvino.ai/)
- [OpenVINO GenAI](https://docs.openvino.ai/2025/openvino-workflow-generative/inference-with-genai.html)
- [Intel NPU 文档](https://docs.openvino.ai/2025/openvino-workflow-generative/inference-with-genai/inference-with-genai-on-npu.html)
- [Intel Ollama](https://www.modelscope.cn/models/Intel/ollama)

### 模型资源
- [Qwen 官方仓库](https://github.com/QwenLM/Qwen)
- [ModelScope 模型库](https://www.modelscope.cn/)
- [HuggingFace 模型库](https://huggingface.co/)

### 驱动下载
- [Intel Arc 驱动](https://www.intel.com/content/www/us/en/download/785597/intel-arc-iris-xe-graphics-windows.html)
- [Intel NPU 驱动](https://www.intel.com/content/www/us/en/download-center/home.html)

## 📝 开发路线图

### 已完成 ✅
- [x] Intel Ollama 集成
- [x] OpenVINO NPU 部署
- [x] Qwen3 系列模型支持
- [x] INT4/INT8 量化
- [x] MoE 模型优化
- [x] 一键部署脚本
- [x] 多模态支持（视觉、语音）

### 进行中 🚧
- [ ] 更多模型支持（Llama 3, Mistral）
- [ ] Web UI 界面
- [ ] API 服务封装
- [ ] Docker 容器化

### 计划中 📋
- [ ] 模型微调工具
- [ ] RAG（检索增强生成）集成
- [ ] 多语言支持优化
- [ ] 性能监控和分析工具

## 🤝 贡献指南

欢迎贡献代码、文档或反馈问题！

### 如何贡献
1. Fork 本项目
2. 创建特性分支 (`git checkout -b feature/AmazingFeature`)
3. 提交更改 (`git commit -m 'Add some AmazingFeature'`)
4. 推送到分支 (`git push origin feature/AmazingFeature`)
5. 开启 Pull Request

### 报告问题
- 使用 GitHub Issues 报告 bug
- 提供详细的错误信息和复现步骤
- 包含系统信息（GPU 型号、驱动版本等）

## 📄 许可证

本项目采用 Apache 2.0 许可证。详见 LICENSE 文件。

### 第三方许可
- Qwen 模型: Apache 2.0
- OpenVINO: Apache 2.0
- Ollama: MIT License

## 👥 致谢

- **Intel**: 提供 OpenVINO 和 Intel Ollama
- **Qwen Team**: 开发优秀的 Qwen 系列模型
- **HuggingFace**: 提供模型托管和工具
- **ModelScope**: 提供国内模型下载服务

## 📞 联系方式

- **问题反馈**: 使用 GitHub Issues
- **技术讨论**: 参与 Discussions
- **商业合作**: 联系项目维护者

---

## 🎉 快速开始命令

```cmd
# 最简单的方式（推荐）
setup_qwen3_auto.bat

# 或者使用 Python
python deploy_qwen3_npu.py

# 或者手动启动
cd ollama-intel-2.3.0b20250923-win
start-ollama.bat
ollama run qwen3:30b
```

**祝你使用愉快！** 🚀

---

*最后更新: 2025-12-11*
*项目版本: 1.0.0*
